// performance/inference_tester.dart
import 'dart:async';
import 'dart:convert';
import 'dart:io';
import 'dart:math';
import 'dart:typed_data';
import 'package:image/image.dart' as img;
import 'package:onnxruntime/onnxruntime.dart';

import 'device_info.dart';
import 'inference_metrics.dart';

class InferenceTester {
  final OrtSession session;
  final Uint8List testImageBytes;

  List<int> _inferenceTimesMs = [];
  List<int> _preprocessTimesMs = [];
  List<int> _postprocessTimesMs = [];

  InferenceTester({required this.session, required this.testImageBytes});

  /// Chu·∫©n b·ªã input tensor (c√πng logic v·ªõi OnnxPredictor)
  Future<Map<String, OrtValue>> _prepareInput() async {
    final preprocessStopwatch = Stopwatch()..start();

    // 1. Decode ·∫£nh
    final image = img.decodeImage(testImageBytes)!;

    // 2. Resize v·ªÅ 224x224
    final resized = img.copyResize(image, width: 224, height: 224);

    // 3. T·∫°o tensor (1, 3, 224, 224)
    final input = Float32List(1 * 3 * 224 * 224);
    int index = 0;

    for (int c = 0; c < 3; c++) {
      for (int y = 0; y < 224; y++) {
        for (int x = 0; x < 224; x++) {
          final pixel = resized.getPixel(x, y);

          double value;
          if (c == 0) {
            value = pixel.r / 255.0;
          } else if (c == 1) {
            value = pixel.g / 255.0;
          } else {
            value = pixel.b / 255.0;
          }

          input[index++] = value; // Kh√¥ng chu·∫©n h√≥a ƒë·ªÉ ƒë∆°n gi·∫£n h√≥a test
        }
      }
    }

    // 4. T·∫°o OrtValueTensor
    final inputTensor = OrtValueTensor.createTensorWithDataList(input, [
      1,
      3,
      224,
      224,
    ]);

    preprocessStopwatch.stop();
    _preprocessTimesMs.add(preprocessStopwatch.elapsedMilliseconds);

    return {'images': inputTensor};
  }

  /// Th·ª±c hi·ªán warmup inference
  Future<void> performWarmup({int runs = 3}) async {
    print('üî• Starting warmup ($runs runs)...');

    final testInput = await _prepareInput();
    final runOptions = OrtRunOptions();

    for (int i = 0; i < runs; i++) {
      final stopwatch = Stopwatch()..start();
      await session.runAsync(runOptions, testInput);
      stopwatch.stop();

      print('  Warmup ${i + 1}: ${stopwatch.elapsedMilliseconds}ms');

      // Ngh·ªâ gi·ªØa c√°c l·∫ßn ch·∫°y
      if (i < runs - 1) {
        await Future.delayed(Duration(milliseconds: 100));
      }
    }

    print('‚úÖ Warmup completed\n');
  }

  /// Th·ª±c hi·ªán ƒëo inference time
  Future<InferenceMetrics> performMeasurement({int runs = 10}) async {
    print('üìä Starting inference measurement ($runs runs)...');

    _inferenceTimesMs.clear();

    // Chu·∫©n b·ªã input 1 l·∫ßn duy nh·∫•t ƒë·ªÉ test c√¥ng b·∫±ng
    final testInput = await _prepareInput();
    final runOptions = OrtRunOptions();

    for (int i = 0; i < runs; i++) {
      final stopwatch = Stopwatch()..start();

      // Inference (gi·ªëng logic trong OnnxPredictor)
      final dynamic asyncResults = await session.runAsync(
        runOptions,
        testInput,
      );

      // Post-processing
      final postprocessStopwatch = Stopwatch()..start();

      // T√¨m OrtValue ƒë·∫ßu ti√™n trong c·∫•u tr√∫c tr·∫£ v·ªÅ
      OrtValue? firstOrt;
      if (asyncResults is List && asyncResults.isNotEmpty) {
        final firstContainer = asyncResults[0];
        if (firstContainer is List && firstContainer.isNotEmpty) {
          firstOrt = firstContainer[0] as OrtValue?;
        } else if (firstContainer is OrtValue) {
          firstOrt = firstContainer as OrtValue?;
        }
      }

      if (firstOrt == null) {
        throw Exception('No output from runAsync');
      }

      // Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ (ch·ªâ ƒë·ªÉ ƒë·∫£m b·∫£o x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß)
      final dynamic raw = firstOrt.value;
      List<double> outputList;

      if (raw is Float32List) {
        outputList = raw.map((e) => e.toDouble()).toList();
      } else if (raw is List<double>) {
        outputList = List<double>.from(raw);
      } else if (raw is List && raw.isNotEmpty) {
        final first = raw.first;
        if (first is List) {
          outputList = first.map((e) => (e as num).toDouble()).toList();
        } else if (first is num) {
          outputList = raw.map((e) => (e as num).toDouble()).toList();
        } else {
          outputList = [];
        }
      } else {
        outputList = [];
      }

      postprocessStopwatch.stop();
      stopwatch.stop();

      // L∆∞u k·∫øt qu·∫£
      _inferenceTimesMs.add(stopwatch.elapsedMilliseconds);
      _postprocessTimesMs.add(postprocessStopwatch.elapsedMilliseconds);

      print(
        '  Run ${i + 1}: ${stopwatch.elapsedMilliseconds}ms '
        '(Inference: ${stopwatch.elapsedMilliseconds - postprocessStopwatch.elapsedMilliseconds}ms, '
        'Post-process: ${postprocessStopwatch.elapsedMilliseconds}ms)',
      );

      // Ngh·ªâ gi·ªØa c√°c l·∫ßn ch·∫°y
      if (i < runs - 1) {
        await Future.delayed(Duration(milliseconds: 200));
      }
    }

    // T√≠nh to√°n th·ªëng k√™
    final metrics = _calculateMetrics();
    _printReport(metrics);

    return metrics;
  }

  InferenceMetrics _calculateMetrics() {
    if (_inferenceTimesMs.isEmpty) {
      return InferenceMetrics.empty();
    }

    final totalRuns = _inferenceTimesMs.length;

    // Inference time
    final inferenceAvg = _inferenceTimesMs.reduce((a, b) => a + b) ~/ totalRuns;
    final inferenceMin = _inferenceTimesMs.reduce((a, b) => a < b ? a : b);
    final inferenceMax = _inferenceTimesMs.reduce((a, b) => a > b ? a : b);

    // Preprocess time
    final preprocessAvg =
        _preprocessTimesMs.isNotEmpty
            ? _preprocessTimesMs.reduce((a, b) => a + b) ~/
                _preprocessTimesMs.length
            : 0;

    // Postprocess time
    final postprocessAvg =
        _postprocessTimesMs.isNotEmpty
            ? _postprocessTimesMs.reduce((a, b) => a + b) ~/
                _postprocessTimesMs.length
            : 0;

    // T√≠nh ƒë·ªô l·ªách chu·∫©n
    final variance =
        _inferenceTimesMs
            .map((time) => pow(time - inferenceAvg, 2))
            .reduce((a, b) => a + b) /
        totalRuns;
    final stdDev = sqrt(variance).round();

    return InferenceMetrics(
      totalRuns: totalRuns,
      inferenceAvgMs: inferenceAvg,
      inferenceMinMs: inferenceMin,
      inferenceMaxMs: inferenceMax,
      inferenceStdDevMs: stdDev,
      preprocessAvgMs: preprocessAvg,
      postprocessAvgMs: postprocessAvg,
      allTimesMs: List.from(_inferenceTimesMs),
      fps: 1000 / inferenceAvg,
    );
  }

  void _printReport(InferenceMetrics metrics) {
    print('''
üìà ========== INFERENCE PERFORMANCE REPORT ==========
üîÑ Samples: ${metrics.totalRuns} runs
‚è±Ô∏è  Total Average: ${metrics.inferenceAvgMs}ms (${metrics.fps.toStringAsFixed(1)} FPS)
‚îú‚îÄ‚îÄ Preprocessing: ${metrics.preprocessAvgMs}ms
‚îú‚îÄ‚îÄ Core Inference: ${metrics.inferenceAvgMs - metrics.preprocessAvgMs - metrics.postprocessAvgMs}ms
‚îî‚îÄ‚îÄ Postprocessing: ${metrics.postprocessAvgMs}ms
üìä Range: ${metrics.inferenceMinMs}ms - ${metrics.inferenceMaxMs}ms (¬±${metrics.inferenceStdDevMs}ms)
üìã All times: ${metrics.allTimesMs.join(', ')}ms
===================================================
''');
  }
}
